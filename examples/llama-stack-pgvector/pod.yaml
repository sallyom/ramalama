apiVersion: v1
kind: Pod
metadata:
  name: ls-rag-pgvector
  labels:
    app: ls-rag-pgvector
spec:
  hostNetwork: false
  containers:
  - name: pgvector
    image: quay.io/sallyom/pgrag:latest
    securityContext:
      seLinuxOptions:
        type: spc_t
    ports:
    - containerPort: 5432
      hostPort: 5432
    env:
    - name: POSTGRES_HOST
      value: "127.0.0.1"
    - name: POSTGRES_PORT
      value: "5432"
    - name: PGVECTOR_DBNAME
      value: "ragdb"
    - name: POSTGRES_USER
      value: "postgres"
    - name: POSTGRES_PASSWORD
      value: "postgres"
    volumeMounts:
    - name: shm
      mountPath: /dev/shm
    readinessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec pg_isready -U postgres -d ragdb -h 127.0.0.1 -p 5432
      initialDelaySeconds: 5
      periodSeconds: 10
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1000m"

  - name: llama-stack
    image: quay.io/sallyom/ramalama-stack:ubi10
    securityContext:
      seLinuxOptions:
        type: spc_t
    ports:
    - containerPort: 8321
      hostPort: 8321
    env:
    - name: POSTGRES_HOST
      value: "127.0.0.1"
    - name: POSTGRES_PORT
      value: "5432"
    - name: PGVECTOR_DBNAME
      value: "ragdb"
    - name: POSTGRES_USER
      value: "postgres"
    - name: POSTGRES_PASSWORD
      value: "postgres"
    - name: INFERENCE_MODEL
      value: "granite3.3"
    - name: TAVILY_SEARCH_API_KEY
      value: ""
    - name: RAMALAMA_URL
      value: "http://127.0.0.1:8080"
    - name: SQLITE_STORE_DIR
      value: "/tmp/llama-stack"
    volumeMounts:
    - name: llama-stack-config
      mountPath: /etc/ramalama/ramalama-run.yaml
      subPath: ramalama-run.yaml
      readOnly: true
    - name: shm
      mountPath: /dev/shm
    depends_on:
      - pgvector
    readinessProbe:
      httpGet:
        path: /health
        port: 8321
      initialDelaySeconds: 10
      periodSeconds: 30
    resources:
      requests:
        memory: "1Gi"
        cpu: "1000m"
      limits:
        memory: "2Gi"
        cpu: "2000m"

  - name: ramalama-model
    image: quay.io/sallyom/ramalama:ubi10
    ports:
    - containerPort: 8080
      hostPort: 8080
    command:
    - ramalama
    - serve
    - --port=8080
    - --host=0.0.0.0
    - granite3.3
    resources:
      requests:
        memory: "12Gi"
        cpu: "2000m"
      limits:
        memory: "16Gi"
        cpu: "4000m"
    readinessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 30
      periodSeconds: 30

  - name: ui
    image: quay.io/sallyom/ramalama-stack:ui
    ports:
    - containerPort: 8501
      hostPort: 8501
    securityContext:
      seLinuxOptions:
        type: spc_t
    env:
    - name: LLAMA_STACK_ENDPOINT
      value: "http://localhost:8321"
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "500m"
    readinessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 30
      periodSeconds: 30
  volumes:
  - name: llama-stack-config
    configMap:
      name: llama-stack-config
  - name: shm
    emptyDir:
      medium: Memory
      sizeLimit: 1Gi
  - name: llama-stack-data
    emptyDir: {}

  restartPolicy: Always
