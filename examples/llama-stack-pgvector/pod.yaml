apiVersion: v1
kind: Pod
metadata:
  name: llama-stack-pgvector
  labels:
    app: llama-stack-pgvector
spec:
  hostNetwork: false
  containers:
  - name: pgvector
    image: docker.io/pgvector/pgvector:pg17
    ports:
    - containerPort: 5432
      hostPort: 5432
    env:
    - name: POSTGRES_USER
      value: postgres
    - name: POSTGRES_PASSWORD
      value: rag_password
    - name: POSTGRES_DB
      value: rag_blueprint
    - name: POSTGRES_PORT
      value: "5432"
    - name: POSTGRES_DBNAME
      value: rag_blueprint
    volumeMounts:
    - name: init-sql
      mountPath: /docker-entrypoint-initdb.d/init-db.sh
      readOnly: true
    - name: pg-data
      mountPath: /var/lib/postgresql/data
    readinessProbe:
      exec:
        command:
          - /bin/sh
          - -c
          - exec pg_isready -U postgres -d rag_blueprint -h 127.0.0.1 -p 5432
      initialDelaySeconds: 5
      periodSeconds: 10
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "500m"
  
  - name: llama-stack
    image: quay.io/sallyom/ramalama-stack:latest
    ports:
    - containerPort: 8321
      hostPort: 8321
    env:
    - name: POSTGRES_HOST
      value: "127.0.0.1"
    - name: POSTGRES_PORT
      value: "5432"
    - name: PGVECTOR_DBNAME
      value: "rag_blueprint"
    - name: POSTGRES_USER
      value: "postgres"
    - name: POSTGRES_PASSWORD
      value: "rag_password"
    - name: INFERENCE_MODEL
      value: "tinyllama"
    - name: TAVILY_SEARCH_API_KEY
      value: ""
    - name: RAMALAMA_URL
      value: "http://127.0.0.1:8080"
    - name: SQLITE_STORE_DIR
      value: "/tmp/llama-stack"
    volumeMounts:
    - name: config
      mountPath: /etc/ramalama/ramalama-run.yaml
      readOnly: true
    - name: llama-stack-data
      mountPath: /tmp/llama-stack
    depends_on:
      - pgvector
    readinessProbe:
      httpGet:
        path: /health
        port: 8321
      initialDelaySeconds: 10
      periodSeconds: 30
    resources:
      requests:
        memory: "512Mi"
        cpu: "200m"
      limits:
        memory: "1Gi"
        cpu: "1000m"

  - name: ramalama-model
    image: quay.io/sallyom/ramalama:latest
    ports:
    - containerPort: 8080
      hostPort: 8080
    command:
    - ramalama
    - serve
    - --port=8080
    - --host=0.0.0.0
    - tinyllama
    resources:
      requests:
        memory: "4Gi"
        cpu: "1000m"
      limits:
        memory: "8Gi"
        cpu: "2000m"
    readinessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 30
      periodSeconds: 30

  - name: ui
    image: quay.io/sallyom/ramalama-stack:ui
    ports:
    - containerPort: 8501
      hostPort: 8501
    env:
    - name: LLAMA_STACK_ENDPOINT
      value: "http://localhost:8321"
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "500m"

  volumes:
  - name: init-sql
    hostPath:
      path: /Users/somalley/git/containers/ramalama/examples/llama-stack-pgvector/init-db.sh
  - name: config
    hostPath:
      path: /Users/somalley/git/containers/ramalama/examples/llama-stack-pgvector/ramalama-run.yaml
  - name: pg-data
    emptyDir: {}
  - name: llama-stack-data
    emptyDir: {}

  restartPolicy: Always
